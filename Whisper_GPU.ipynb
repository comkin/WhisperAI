{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1897925d-bfbd-45ab-b72b-d94ced8cd32b",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install git+https://github.com/openai/whisper.git\n",
    "\n",
    "!pip install tqdm\n",
    "\n",
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee079b3",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We used Python 3.9.9 and [PyTorch](https://pytorch.org/) 1.10.1 to train and test our models, but the codebase is expected to be compatible with Python 3.8-3.11 and recent PyTorch versions. The codebase also depends on a few Python packages, most notably [OpenAI's tiktoken](https://github.com/openai/tiktoken) for their fast tokenizer implementation. You can download and install (or update to) the latest release of Whisper with the following command:\n",
    "\n",
    "    pip install -U openai-whisper\n",
    "\n",
    "Alternatively, the following command will pull and install the latest commit from this repository, along with its Python dependencies:\n",
    "\n",
    "    pip install git+https://github.com/openai/whisper.git \n",
    "\n",
    "To update the package to the latest version of this repository, please run:\n",
    "\n",
    "    pip install --upgrade --no-deps --force-reinstall git+https://github.com/openai/whisper.git\n",
    "\n",
    "It also requires the command-line tool [`ffmpeg`](https://ffmpeg.org/) to be installed on your system, which is available from most package managers:\n",
    "\n",
    "```bash\n",
    "# on Ubuntu or Debian\n",
    "sudo apt update && sudo apt install ffmpeg\n",
    "\n",
    "# on Arch Linux\n",
    "sudo pacman -S ffmpeg\n",
    "\n",
    "# on MacOS using Homebrew (https://brew.sh/)\n",
    "brew install ffmpeg\n",
    "\n",
    "# on Windows using Chocolatey (https://chocolatey.org/)\n",
    "choco install ffmpeg\n",
    "\n",
    "# on Windows using Scoop (https://scoop.sh/)\n",
    "scoop install ffmpeg\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e1b080f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\W.Mauti\\Documents\\Anaconda\\envs\\AI_test\\Lib\\site-packages\\whisper\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import whisper\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "torch.cuda.is_available()\n",
    "print(whisper.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4f5e10c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the folder where the wav files are located\n",
    "root_folder = \"C:/Users/W.Mauti/Documents/Jupiter\"\n",
    "exit_folder = \"E:\\\\SUBRIZI\\\\Lez 1 02_10\"\n",
    "audio_ext = \".m4a\" # .mp3, .wav, .flac ...\n",
    "language = \"it\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccb4edd",
   "metadata": {},
   "source": [
    "## Available models and languages\n",
    "\n",
    "There are five model sizes, four with English-only versions, offering speed and accuracy tradeoffs. Below are the names of the available models and their approximate memory requirements and inference speed relative to the large model; actual speed may vary depending on many factors including the available hardware.\n",
    "\n",
    "|  Size  | Parameters | English-only model | Multilingual model | Required VRAM | Relative speed |\n",
    "|:------:|:----------:|:------------------:|:------------------:|:-------------:|:--------------:|\n",
    "|  tiny  |    39 M    |     `tiny.en`      |       `tiny`       |     ~1 GB     |      ~32x      |\n",
    "|  base  |    74 M    |     `base.en`      |       `base`       |     ~1 GB     |      ~16x      |\n",
    "| small  |   244 M    |     `small.en`     |      `small`       |     ~2 GB     |      ~6x       |\n",
    "| medium |   769 M    |    `medium.en`     |      `medium`      |     ~5 GB     |      ~2x       |\n",
    "| large  |   1550 M   |        N/A         |      `large`       |    ~10 GB     |       1x       |\n",
    "\n",
    "The `.en` models for English-only applications tend to perform better, especially for the `tiny.en` and `base.en` models. We observed that the difference becomes less significant for the `small.en` and `medium.en` models.\n",
    "\n",
    "Whisper's performance varies widely depending on the language. The figure below shows a performance breakdown of `large-v3` and `large-v2` models by language, using WERs (word error rates) or CER (character error rates, shown in *Italic*) evaluated on the Common Voice 15 and Fleurs datasets. Additional WER/CER metrics corresponding to the other models and datasets can be found in Appendix D.1, D.2, and D.4 of [the paper](https://arxiv.org/abs/2212.04356), as well as the BLEU (Bilingual Evaluation Understudy) scores for translation in Appendix D.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79e0eb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading whisper model...\n",
      "Whisper model complete.\n"
     ]
    }
   ],
   "source": [
    "# Set up Whisper client\n",
    "print(\"Loading whisper model...\")\n",
    "model = whisper.load_model(\"medium\", device=\"cuda\")\n",
    "print(\"Whisper model complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "071c0a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting number of files to transcribe...\n",
      "Number of files:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 611756/611756 [47:30<00:00, 214.64frames/s]\n",
      "Transcribing Files: 100%|██████████| 1/1 [47:55<00:00, 2875.42s/it]\n"
     ]
    }
   ],
   "source": [
    "# Get the number of wav files in the root folder and its sub-folders\n",
    "print(\"Getting number of files to transcribe...\")\n",
    "num_files = sum(1 for dirpath, dirnames, filenames in os.walk(root_folder) for filename in filenames if filename.endswith(audio_ext))\n",
    "print(\"Number of files: \", num_files)\n",
    "\n",
    "\n",
    "# Transcribe the wav files and display a progress bar\n",
    "with tqdm(total=num_files, desc=\"Transcribing Files\") as pbar:\n",
    "    for dirpath, dirnames, filenames in os.walk(root_folder):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith(audio_ext):\n",
    "                filepath = str(os.path.join(dirpath, filename))\n",
    "                result = model.transcribe(filepath, language=language, fp16=False, verbose=False)\n",
    "                transcription = result['text']\n",
    "                # Write transcription to text file\n",
    "                filename_no_ext = os.path.splitext(filename)[0]\n",
    "                with open(os.path.join(exit_folder, filename_no_ext + '.txt'), 'w') as f:\n",
    "                    f.write(transcription)\n",
    "                pbar.update(1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
